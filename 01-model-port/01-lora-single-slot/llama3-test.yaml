name: mistral lora easy - 1x slot

workspace: kdev
project: llm

bind_mounts:
  - container_path: /root/.cache/huggingface
    host_path: /root/.cache/huggingface
    propagation: rprivate
    read_only: false

debug: false
environment:
  environment_variables:
    - NCCL_DEBUG=INFO
  image: 
    gpu: determinedai/environments:cuda-11.8-pytorch-2.0-gpu-95c7a14
    cpu: determinedai/environments:py-3.10-pytorch-2.0-cpu-03ae7d7
resources:
  slots_per_trial: 1
searcher:
  name: single
  max_length:
    batches: 5000
  metric: eval_accuracy
  smaller_is_better: false
hyperparameters:
  model: "meta-llama/Meta-Llama-3-8B-Instruct"

  token: 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

  dataset_subset: "easy"
  lora: true
  training_args:
    output_dir: "/tmp/llm_finetuning"
    max_steps: 5000
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 4
    bf16: true
    evaluation_strategy: "steps"
    eval_steps: 1000
    logging_strategy: "steps"
    logging_steps: 100
    save_strategy: "steps"
    save_steps: 1000
    learning_rate: 1e-5
entrypoint: >-
  python -m determined.launch.torch_distributed
  python finetune.py
max_restarts: 0